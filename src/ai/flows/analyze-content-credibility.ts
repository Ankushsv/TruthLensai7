// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Analyzes the credibility of content (text or a link) using AI.
 *
 * - analyzeContentCredibility - A function that analyzes the credibility of content.
 * - AnalyzeContentCredibilityInput - The input type for the analyzeContentCredibility function.
 * - AnalyzeContentCredibilityOutput - The return type for the analyzeContentCredibility function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import { retrieveEvidence } from '@/ai/tools/evidence-retriever';

const AnalyzeContentCredibilityInputSchema = z.object({
  content: z.string().describe('The text or link to analyze for credibility.'),
});
export type AnalyzeContentCredibilityInput = z.infer<typeof AnalyzeContentCredibilityInputSchema>;

const AnalyzeContentCredibilityOutputSchema = z.object({
  credibilityScore: z.number().describe('A score indicating the credibility of the content (0-1).'),
  explanation: z.string().describe('A detailed explanation of the credibility analysis, highlighting specific aspects that contribute to the assessment.'),
  flags: z.array(z.string()).describe('Specific issues or concerns raised by the analysis (e.g., "clickbait", "bias", "fake claim").'),
  sourcesChecked: z.array(z.string()).describe('A list of sources or types of sources checked (e.g., "fact-check databases", "major news outlets").'),
  factCheckReferences: z.array(z.string()).describe('A list of URLs to external fact-check articles or other references that support the analysis.'),
});
export type AnalyzeContentCredibilityOutput = z.infer<typeof AnalyzeContentCredibilityOutputSchema>;

export async function analyzeContentCredibility(input: AnalyzeContentCredibilityInput): Promise<AnalyzeContentCredibilityOutput> {
  return analyzeContentCredibilityFlow(input);
}

const analyzeContentCredibilityPrompt = ai.definePrompt({
  name: 'analyzeContentCredibilityPrompt',
  input: {schema: AnalyzeContentCredibilityInputSchema},
  output: {schema: AnalyzeContentCredibilityOutputSchema},
  tools: [retrieveEvidence],
  prompt: `You are an AI assistant designed to analyze the credibility of text or links provided by the user.

  Your goal is to determine a credibility score and provide a detailed analysis based on retrieved evidence.

  Follow these steps:
  1.  Examine the user's content to identify the core claim being made.
  2.  If the content contains a specific, verifiable claim, you MUST use the 'retrieveEvidence' tool to gather factual evidence from external knowledge sources.
  3.  Analyze the user's content strictly in conjunction with the retrieved evidence snippets. Do not rely on your own internal knowledge.
  4.  Based on the evidence, determine the credibility score, provide an explanation, and list any issues, sources checked, and fact-checking URLs.

  Content to analyze: {{{content}}}

  - For 'flags', use terms like "clickbait", "strong emotional language", "potential bias", "unverified claim", "conflicts with evidence".
  - For 'sourcesChecked', list the categories of sources you are simulating checking, like "major news outlets", "scientific journals", "fact-checking websites", and crucially, include "External Knowledge Base via retrieveEvidence" if you used the tool.
  - For 'factCheckReferences', if the retrieved evidence provides specific, relevant URLs, include them. If not, provide an empty array.

  Respond in a JSON format.
  `,
});

const analyzeContentCredibilityFlow = ai.defineFlow(
  {
    name: 'analyzeContentCredibilityFlow',
    inputSchema: AnalyzeContentCredibilityInputSchema,
    outputSchema: AnalyzeContentCredibilityOutputSchema,
  },
  async input => {
    const {output} = await analyzeContentCredibilityPrompt(input);
    return output!;
  }
);
