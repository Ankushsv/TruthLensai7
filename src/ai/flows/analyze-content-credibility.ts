
// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Analyzes the credibility of content (text or a link) using AI.
 *
 * - analyzeContentCredibility - A function that analyzes the credibility of content.
 * - AnalyzeContentCredibilityInput - The input type for the analyzeContentCredibility function.
 * - AnalyzeContentCredibilityOutput - The return type for the analyzeContentCredibility function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import { retrieveEvidence } from '@/ai/tools/evidence-retriever';

const AnalyzeContentCredibilityInputSchema = z.object({
  content: z.string().describe('The text or link to analyze for credibility.'),
});
export type AnalyzeContentCredibilityInput = z.infer<typeof AnalyzeContentCredibilityInputSchema>;

const AnalyzeContentCredibilityOutputSchema = z.object({
  credibilityScore: z.number().describe('A score indicating the credibility of the content (0-1).'),
  explanation: z.string().describe('A detailed explanation of the credibility analysis, highlighting specific aspects that contribute to the assessment.'),
  flags: z.array(z.string()).describe('Specific issues or concerns raised by the analysis (e.g., "clickbait", "bias", "fake claim").'),
  sourcesChecked: z.array(z.string()).describe('A list of sources or types of sources checked (e.g., "fact-check databases", "major news outlets").'),
  factCheckReferences: z.array(z.string()).describe('A list of URLs to external fact-check articles or other references that support the analysis.'),
});
export type AnalyzeContentCredibilityOutput = z.infer<typeof AnalyzeContentCredibilityOutputSchema>;

// Define an extended schema that includes the evidence for the prompt
const
  PromptWithEvidenceSchema = AnalyzeContentCredibilityInputSchema.extend({
  evidence: z.array(z.string()).describe('A list of evidence snippets to consider.'),
});

export async function analyzeContentCredibility(input: AnalyzeContentCredibilityInput): Promise<AnalyzeContentCredibilityOutput> {
  return analyzeContentCredibilityFlow(input);
}

const analyzeContentCredibilityPrompt = ai.definePrompt({
  name: 'analyzeContentCredibilityPrompt',
  input: {schema: PromptWithEvidenceSchema},
  output: {schema: AnalyzeContentCredibilityOutputSchema},
  prompt: `You are an AI assistant designed to analyze the credibility of text or links provided by the user.

  Your goal is to determine a credibility score and provide a detailed analysis based on retrieved evidence.

  Analyze the user's content strictly in conjunction with the retrieved evidence snippets. Do not rely on your own internal knowledge.
  Based on the evidence, determine the credibility score, provide an explanation, and list any issues, sources checked, and fact-checking URLs.

  Content to analyze: {{{content}}}

  Evidence snippets:
  {{#each evidence}}
  - {{{this}}}
  {{/each}}

  - For 'flags', use terms like "clickbait", "strong emotional language", "potential bias", "unverified claim", "conflicts with evidence".
  - For 'sourcesChecked', list the categories of sources you are simulating checking, like "major news outlets", "scientific journals", "fact-checking websites", and crucially, include "External Knowledge Base via retrieveEvidence".
  - For 'factCheckReferences', if the retrieved evidence provides specific, relevant URLs, include them. If not, provide an empty array.

  Respond in a JSON format.
  `,
});

const analyzeContentCredibilityFlow = ai.defineFlow(
  {
    name: 'analyzeContentCredibilityFlow',
    inputSchema: AnalyzeContentCredibilityInputSchema,
    outputSchema: AnalyzeContentCredibilityOutputSchema,
  },
  async (input) => {
    // Step 1: Retrieve evidence based on the content's core claim.
    // In a real app, you might first have an LLM extract the "claim" from the content.
    // For this prototype, we'll use the whole content as the claim.
    const { evidence } = await retrieveEvidence({ claim: input.content });

    // Step 2: Run the analysis prompt, providing both the content and the retrieved evidence.
    const { output } = await analyzeContentCredibilityPrompt({
        ...input,
        evidence: evidence,
    });
    
    return output!;
  }
);
